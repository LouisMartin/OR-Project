{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA on MS COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.applications import vgg19\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from image_processing import load_images, categories, ann_file\n",
    "from vgg import compute_nn_features\n",
    "from text_processing import create_caption_dataframe\n",
    "from word2vec import compute_textual_features\n",
    "from tools import intersect_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Visual features and Textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coco = COCO(ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_visual, visual_img_ids = load_images(categories, coco=coco)\n",
    "#np.save('X_visual.npy', X_visual)\n",
    "#np.save('visual_img_ids.npy', visual_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_visual = np.load('X_visual.npy')\n",
    "visual_img_ids = np.load('visual_img_ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = vgg19.VGG19()\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "net.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute visual features batch by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = np.zeros((X_visual.shape[0], 4096))\n",
    "for i in tqdm(range(X_visual.shape[0]//10+1)):\n",
    "    start_index = (i)*10\n",
    "    end_index = (i+1)*10\n",
    "    end_index = min(end_index, X_visual.shape[0])\n",
    "    X_temp = X_visual[start_index:end_index]\n",
    "    V_temp = compute_nn_features(X_temp, net, layer=2)\n",
    "    V[start_index:end_index,:] = V_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('V.npy', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = np.load('V.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_caption = create_caption_dataframe()\n",
    "T = compute_textual_features(df_caption)\n",
    "textual_img_ids = df_caption.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take only the features corresponding to common ids and sort by id\n",
    "V, visual_img_ids, T, textual_img_ids = intersect_sort(V, visual_img_ids, T, textual_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Quoted form the CCA paper:\n",
    "We search a range from 16 to 1,024, doubling the\n",
    "dimensionality each time, and the resulting values typically\n",
    "fall around 128-256 on all our datasets.'''\n",
    "d = 128 # Dimension of the final joint latent space\n",
    "cca = CCA(n_components=d, scale=False)\n",
    "cca.fit(V,T)\n",
    "\n",
    "# New basis projection matrices\n",
    "W1 = cca.x_weights_\n",
    "W2 = cca.y_weights_\n",
    "\n",
    "# Compute features in the new latent space\n",
    "V_latent = np.dot(V,W1)\n",
    "T_latent = np.dot(T,W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('W1.npy', W1)\n",
    "#np.save('W2.npy', W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot latent space with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "tic = time.time()\n",
    "embeddings = tsne.fit_transform(np.vstack((V_latent, T_latent)))\n",
    "print(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the categories of each image\n",
    "cat_ids = coco.getCatIds(catNms=categories)\n",
    "\n",
    "df = pd.DataFrame(index=visual_img_ids, columns=['cat_id'])\n",
    "for cat_id in cat_ids:\n",
    "    img_ids = coco.getImgIds(catIds=cat_id)\n",
    "    for img_id in img_ids:\n",
    "        df.loc[img_id] = cat_id\n",
    "df = df.loc[visual_img_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the t-SNE embeddings by category (color) and visula/textual (shape)\n",
    "Vx = embeddings[:V.shape[0],0]\n",
    "Vy = embeddings[:V.shape[0],1]\n",
    "Tx = embeddings[V.shape[0]:,0]\n",
    "Ty = embeddings[V.shape[0]:,1]\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "for cat_id, color in zip(cat_ids, colors):\n",
    "    idx = (df['cat_id'] == cat_id).as_matrix()\n",
    "    plt.plot(Vx[idx], Vy[idx], color+'o', markersize=4)\n",
    "    plt.plot(Tx[idx], Ty[idx], color+'^')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
