{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA on MS COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import os.path as op\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.applications import vgg19\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from image_processing import load_images, categories, ann_file\n",
    "from vgg import compute_nn_features_ids\n",
    "from text_processing import create_caption_dataframe\n",
    "from word2vec import compute_textual_features\n",
    "from tools import intersect_sort, split_ids, get_all_sorted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Visual features and Textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coco = COCO(ann_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual features\n",
    "Compute visual features part by part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = vgg19.VGG19()\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "net.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual_embeddings_folder = 'data/visual_embeddings'\n",
    "if not op.exists(visual_embeddings_folder):\n",
    "    os.makedirs(visual_embeddings_folder)\n",
    "\n",
    "n_parts = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for part in range(n_parts//2, n_parts):\n",
    "    path_embeddings = op.join(visual_embeddings_folder, 'V_{}.npy'.format(part+1))\n",
    "    path_ids = op.join(visual_embeddings_folder, 'processed_ids_{}.npy'.format(part+1))\n",
    "    if (not op.exists(path_embeddings)) and (not op.exists(path_ids)):\n",
    "        print('Part: {}'.format(part+1))\n",
    "        ids = split_ids(part, n_parts, coco)\n",
    "        V, processed_ids = compute_nn_features_ids(ids, net, coco)\n",
    "\n",
    "        # Save embeddings and ids\n",
    "        np.save(path_embeddings, V)\n",
    "        np.save(path_ids, processed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and merge computed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = []\n",
    "processed_ids = []\n",
    "for part in tqdm(range(n_parts)):\n",
    "    path_embeddings = op.join(visual_embeddings_folder, 'V_{}.npy'.format(part+1))\n",
    "    path_ids = op.join(visual_embeddings_folder, 'processed_ids_{}.npy'.format(part+1))\n",
    "    if op.exists(path_embeddings) and op.exists(path_ids):\n",
    "        V.append(np.load(path_embeddings))\n",
    "        processed_ids.append(np.load(path_ids))\n",
    "    else:\n",
    "        raise IOError('Files for part {} not found'.format(part+1))\n",
    "\n",
    "visual_img_ids = np.hstack(processed_ids)\n",
    "V = np.vstack(V)\n",
    "np.save('data/V.npy', V)\n",
    "np.save('data/visual_img_ids', visual_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = np.load('data/V.npy')\n",
    "visual_img_ids = np.load('data/visual_img_ids.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_caption = create_caption_dataframe()\n",
    "T = compute_textual_features(df_caption, overwrite=False)\n",
    "textual_img_ids = df_caption.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all categories (sorted and no duplicates)\n",
    "cat_ids = coco.getCatIds()\n",
    "cat_ids = sorted(set(cat_ids))\n",
    "\n",
    "img_ids = get_all_sorted_ids(coco)\n",
    "df = pd.DataFrame(0, index=img_ids, columns=cat_ids)\n",
    "\n",
    "# For each row set the column corresponding to the image category to one\n",
    "for cat_id in tqdm(cat_ids):\n",
    "    img_ids = coco.getImgIds(catIds=cat_id)\n",
    "    df.loc[img_ids, cat_id] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print image with category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from image_processing import plot_image_by_id\n",
    "\n",
    "img_id = 30\n",
    "\n",
    "# Print category names\n",
    "cat_ids = df.iloc[0,(df.loc[img_id] == 1).values].index.values\n",
    "cats = coco.loadCats(cat_ids.tolist())\n",
    "print([cat['name'] for cat in cats])\n",
    "\n",
    "plot_image_by_id(img_id, coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semantic_img_ids = df.index.values\n",
    "C = df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take only the features corresponding to common ids and sort by id\n",
    "V, visual_img_ids, T, textual_img_ids = intersect_sort(V, visual_img_ids, T, textual_img_ids)\n",
    "V, visual_img_ids, C, semantic_img_ids = intersect_sort(V, visual_img_ids, C, semantic_img_ids)\n",
    "T, textual_img_ids, C, semantic_img_ids = intersect_sort(T, textual_img_ids, C, semantic_img_ids)\n",
    "assert len(V) == len(T) and len(T) == len(C)\n",
    "assert len(visual_img_ids) == len(textual_img_ids) and len(textual_img_ids) == len(semantic_img_ids)\n",
    "assert len(V) == len(visual_img_ids)\n",
    "print('{} images in common kept'.format(len(V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Quoted form the CCA paper:\n",
    "We search a range from 16 to 1,024, doubling the\n",
    "dimensionality each time, and the resulting values typically\n",
    "fall around 128-256 on all our datasets.'''\n",
    "d = 128 # Dimension of the final joint latent space\n",
    "cca = CCA(n_components=d, scale=False)\n",
    "print('Fitting CCA ...')\n",
    "tic = time.time()\n",
    "cca.fit(V,T)\n",
    "print(time.time() - tic)\n",
    "\n",
    "# New basis projection matrices\n",
    "W1 = cca.x_weights_\n",
    "W2 = cca.y_weights_\n",
    "\n",
    "# Compute features in the new latent space\n",
    "V_latent = np.dot(V,W1)\n",
    "T_latent = np.dot(T,W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35878 seconds for 195 components out of 256\n",
    "-> 180 seconds per component (500 iter per component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('W1.npy', W1)\n",
    "#np.save('W2.npy', W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = np.load('W1.npy')\n",
    "W2 = np.load('W2.npy')\n",
    "V_latent = np.dot(V,W1)\n",
    "T_latent = np.dot(T,W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot latent space with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "tic = time.time()\n",
    "embeddings = tsne.fit_transform(np.vstack((V_latent, T_latent)))\n",
    "print(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the categories of each image\n",
    "cat_ids = coco.getCatIds(catNms=categories)\n",
    "\n",
    "df = pd.DataFrame(index=visual_img_ids, columns=['cat_id'])\n",
    "for cat_id in cat_ids:\n",
    "    img_ids = coco.getImgIds(catIds=cat_id)\n",
    "    for img_id in img_ids:\n",
    "        df.loc[img_id] = cat_id\n",
    "df = df.loc[visual_img_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the t-SNE embeddings by category (color) and visula/textual (shape)\n",
    "Vx = embeddings[:V.shape[0],0]\n",
    "Vy = embeddings[:V.shape[0],1]\n",
    "Tx = embeddings[V.shape[0]:,0]\n",
    "Ty = embeddings[V.shape[0]:,1]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "colors = ['r', 'b', 'g']\n",
    "for cat_id, color in zip(cat_ids, colors):\n",
    "    idx = (df['cat_id'] == cat_id).as_matrix()\n",
    "    plt.plot(Vx[idx], Vy[idx], color+'o', markersize=3,\n",
    "             label='Category {} - Visual'.format(cat_id))\n",
    "    plt.plot(Tx[idx], Ty[idx], color+'^', markersize=4,\n",
    "             label='Category {} - Textual'.format(cat_id))\n",
    "plt.legend()\n",
    "plt.title('Latent space - 3 MS COCO categories')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
