{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I2T and T2I search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os.path as op\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import seaborn as sns\n",
    "\n",
    "from retrieval_pipelines import absolute_coco_path, tag_to_image_search, image_to_tag_search, most_common_tags\n",
    "from text_processing import create_caption_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coco API loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"dataset\"\n",
    "data_type = \"train2014\"\n",
    "ann_file = op.join(\n",
    "    data_dir, \"annotations\", \"instances_{0}.json\".format(data_type))\n",
    "coco = COCO(ann_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-calculated features loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_visual = np.load(\"W1_all_2_view.npy\")\n",
    "W_text = np.load(\"W2_all_2_view.npy\")\n",
    "V = np.load(\"data/V_final.npy\")\n",
    "T = np.load(\"T.npy\")\n",
    "img_ids = np.load(\"data/visual_img_ids_final.npy\")\n",
    "W_text = W_text.T\n",
    "W_visual = W_visual.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_eigenvals = np.load(\"top_eigvals_all_2_view.npy\")\n",
    "D = np.diag(top_eigenvals**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag to image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "database_images = W_visual.dot(V.T)\n",
    "database_images = database_images.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag = \"night\"\n",
    "feature_path = \"{0}_features.npy\".format(\"-\".join(tag.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not op.exists(feature_path):\n",
    "    import gensim\n",
    "    \n",
    "    from text_processing import sentence2vec\n",
    "    \n",
    "    print('\\nLoading word2vec model ...')\n",
    "    model_path = op.join('models', 'GoogleNews-vectors-negative300.bin')\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(model_path, binary=True)\n",
    "    tag_features = sentence2vec(tag, model)\n",
    "    del model\n",
    "    np.save(feature_path, tag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_features = np.load(feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retrieved_img_ids = tag_to_image_search(tag_features, W_text, database_images, img_ids, coco, D, n_images=5, distance=\"similarity\")\n",
    "img_paths = [absolute_coco_path(int(img_id), coco) for img_id in retrieved_img_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "for im_idx, img_path in enumerate(img_paths):\n",
    "    plt.figure(im_idx)\n",
    "    img = mpimg.imread(img_path)\n",
    "    imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save images for all tags of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_of_interest = [\"dog skateboard\", \"red\", \"yellow\", \"cute rabbit\", \"cheerleader girls\", \"sail\", \"ocean\", \"ship sunset\",\n",
    "        \"ship\", \"giraffe\", \"sunset\", \"beautiful\", \"proud\", \"jump\", \"jumping\", \"water\", \"mess\", \"crowd\", \"party\"]\n",
    "im_dir = op.join(\"report\", \"euclidean_2_view\")\n",
    "for tag in tags_of_interest:\n",
    "    feature_path = \"{0}_features.npy\".format(\"-\".join(tag.split()))\n",
    "    if not op.exists(feature_path):\n",
    "        import gensim\n",
    "\n",
    "        from text_processing import sentence2vec\n",
    "\n",
    "        print('\\nLoading word2vec model ...')\n",
    "        model_path = op.join('models', 'GoogleNews-vectors-negative300.bin')\n",
    "        model = gensim.models.Word2Vec.load_word2vec_format(model_path, binary=True)\n",
    "        tag_features = sentence2vec(tag, model)\n",
    "        del model\n",
    "        np.save(feature_path, tag_features)\n",
    "    tag_features = np.load(feature_path)\n",
    "    retrieved_img_ids = tag_to_image_search(tag_features, W_text, database_images, img_ids, coco, D, n_images=5, distance=\"euclidean\")\n",
    "    img_paths = [absolute_coco_path(int(img_id), coco) for img_id in retrieved_img_ids]\n",
    "    sns.set_style(\"dark\")\n",
    "    for im_idx, img_path in enumerate(img_paths):\n",
    "        plt.figure(im_idx)\n",
    "        img = mpimg.imread(img_path)\n",
    "        imgplot = plt.imshow(img)\n",
    "        img_path = \"{0}_{1}.png\".format(\"-\".join(tag.split()), im_idx+1)\n",
    "        plt.savefig(op.join(im_dir, img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to tag search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database_captions = W_text.dot(T.T)\n",
    "database_captions = database_captions.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_name = \"giraffe.jpg\"\n",
    "image_path = op.join(data_dir, \"test\", image_name)\n",
    "visual_feature_path = \"{0}_visual_features.npy\".format(image_name.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mpimg.imread(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not op.exists(visual_feature_path):\n",
    "    from keras.applications import vgg19\n",
    "    from keras.optimizers import SGD\n",
    "    \n",
    "    from image_processing import process_image\n",
    "    from vgg import compute_nn_features\n",
    "    \n",
    "    img_mat = process_image(image_path)\n",
    "    net = vgg19.VGG19()\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    net.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    visual_features = compute_nn_features([img_mat], net)\n",
    "    np.save(\"{0}_visual_features\".format(image_name.split(\".\")[0]), visual_features)\n",
    "    del net\n",
    "    del sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual_features = np.load(visual_feature_path)\n",
    "visual_features = np.reshape(visual_features, (W_visual.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_tags = 4\n",
    "annotations = image_to_tag_search(visual_features.T, W_visual, database_captions, img_ids, coco,D,\n",
    "                                  n_tags=n_tags, expanding_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_tags(annotations, n_tags, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_of_interest = [\"giraffe.jpg\", \"boat.jpg\", \"desk.jpg\", \"teddy_bear.jpg\", \"elephant_drawing.jpg\"]\n",
    "for image_name in images_of_interest:\n",
    "    visual_feature_path = \"{0}_visual_features.npy\".format(image_name.split(\".\")[0])\n",
    "    visual_features = np.load(visual_feature_path)\n",
    "    visual_features = np.reshape(visual_features, (W_visual.shape[1]))\n",
    "    n_tags = 4\n",
    "    annotations = image_to_tag_search(visual_features.T, W_visual, database_captions, img_ids, coco,D,\n",
    "                                      n_tags=n_tags, expanding_factor=10)\n",
    "    stops = set(stopwords.words('english'))\n",
    "    mmt = most_common_tags(annotations, n_tags, stops)\n",
    "    print(\"{0}: {1}\".format(image_name, mmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision recall (PR) curve for T2I\n",
    "Given that the caption evaluation is not available in the Python MS Coco API, we chose to check whether an image was correctly returned by seeing if the tag was present in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_caption = create_caption_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = \"giraffe snowboard boat snow water\".split()\n",
    "features_paths = [\"{0}_features.npy\".format(\"-\".join(tag.split())) for tag in tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a pre-processing of all the tags in order not to have to run the forward pass everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tag_idx, tag in enumerate(tags):\n",
    "    import gensim\n",
    "    from text_processing import sentence2vec\n",
    "    if not op.exists(features_paths[tag_idx]):\n",
    "        try:\n",
    "            model\n",
    "        except NameError:\n",
    "            print(\"Loading Word2Vec model\")\n",
    "            model_path = op.join('models', 'GoogleNews-vectors-negative300.bin')\n",
    "            model = gensim.models.Word2Vec.load_word2vec_format(model_path, binary=True)\n",
    "        finally:\n",
    "            tag_features = sentence2vec(tag, model)\n",
    "            np.save(features_paths[tag_idx], tag_features)\n",
    "            del tag_features\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    print(\"its all good man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the recall, we need to first compute the number of images for this tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = dict()\n",
    "for tag in tags:\n",
    "    counter[tag] = 0\n",
    "for caption in df_caption.loc[img_ids][\"caption\"]:\n",
    "    for tag in tags:\n",
    "        counter[tag] += int(all([(sub_tag in caption) for sub_tag in tag.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_n_images = 70000\n",
    "precisions_cosine = dict()\n",
    "recalls_cosine = dict()\n",
    "precisions_euclidean = dict()\n",
    "recalls_euclidean = dict()\n",
    "numbers_of_images = [1, 200, 500, 800, 900, 1000, 1200, 1500, 1700, 2000, 2500, 3000, 3400, 3700, 5000, 10000, 25000, 40000, 50000, 70000]\n",
    "for tag in tags:\n",
    "    precisions_cosine[tag] = np.zeros(len(numbers_of_images))\n",
    "    recalls_cosine[tag] = np.zeros(len(numbers_of_images))\n",
    "    precisions_euclidean[tag] = np.zeros(len(numbers_of_images))\n",
    "    recalls_euclidean[tag] = np.zeros(len(numbers_of_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tag_idx, tag in enumerate(tags):\n",
    "    tag_features = np.load(features_paths[tag_idx])\n",
    "    for i, n_images in enumerate(numbers_of_images):\n",
    "        retrieved_img_ids = tag_to_image_search(tag_features, W_text, database_images, img_ids, coco,D,\n",
    "                                                n_images=n_images)\n",
    "        correct_number = 0\n",
    "        for caption in df_caption.loc[retrieved_img_ids][\"caption\"]:\n",
    "            correct_number += int(all([sub_tag in caption for sub_tag in tag.split()]))\n",
    "        precisions_cosine[tag][i] = correct_number*100 / n_images\n",
    "        recalls_cosine[tag][i] = correct_number*100 / counter[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tag_idx, tag in enumerate(tags):\n",
    "    tag_features = np.load(features_paths[tag_idx])\n",
    "    for i, n_images in enumerate(numbers_of_images):\n",
    "        retrieved_img_ids = tag_to_image_search(tag_features, W_text, database_images, img_ids, coco,D,\n",
    "                                                n_images=n_images, distance=\"euclidean\")\n",
    "        correct_number = 0\n",
    "        for caption in df_caption.loc[retrieved_img_ids][\"caption\"]:\n",
    "            correct_number += int(all([sub_tag in caption for sub_tag in tag.split()]))\n",
    "        precisions_euclidean[tag][i] = correct_number*100 / n_images\n",
    "        recalls_euclidean[tag][i] = correct_number*100 / counter[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "for tag_idx, tag in enumerate(tags):\n",
    "    precision_cosine = precisions_cosine[tag]\n",
    "    recall_cosine = recalls_cosine[tag]\n",
    "    precision_euclidean = precisions_euclidean[tag]\n",
    "    recall_euclidean = recalls_euclidean[tag]\n",
    "    plt.figure(tag_idx)\n",
    "    plt.title(\"Precision-Recall curve for {0}\".format(tag))\n",
    "    plt.plot(recall_cosine, precision_cosine, label=\"cosine distance\")\n",
    "    plt.plot(recall_euclidean, precision_euclidean, label=\"euclidean distance\")\n",
    "    plt.ylabel(\"Precision (%)\")\n",
    "    plt.xlabel(\"Recall (%)\")\n",
    "    plt.legend(loc=3)\n",
    "    plt.axis([0, 100, 0, 100])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
